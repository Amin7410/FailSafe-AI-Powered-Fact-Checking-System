# ./scripts/build_vectordb.py

import wikipediaapi
import chromadb
from sentence_transformers import SentenceTransformer
from langchain.text_splitter import RecursiveCharacterTextSplitter
from tqdm import tqdm
import sys
import os
from factcheck.utils.logger import CustomLogger
# üî• Ph·∫£i ƒë·∫∑t tr∆∞·ªõc khi import factcheck
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

logger = CustomLogger(__name__).getlog()

# --- C·∫§U H√åNH ---
# Danh s√°ch c√°c ch·ªß ƒë·ªÅ tr√™n Wikipedia ƒë·ªÉ t·∫£i v·ªÅ v√† index
WIKI_PAGES = [
    "Cancer"
]
# T√™n c·ªßa collection trong ChromaDB
COLLECTION_NAME = "wikipedia_knowledge"
# ƒê∆∞·ªùng d·∫´n ƒë·ªÉ l∆∞u DB tr√™n ƒëƒ©a
DB_PATH = "./chroma_db"
# T√™n c·ªßa m√¥ h√¨nh embedding
EMBEDDING_MODEL_NAME = 'intfloat/e5-base-v2'


def build_database():
    """
    T·∫£i d·ªØ li·ªáu t·ª´ Wikipedia, chia nh·ªè, t·∫°o vector v√† l∆∞u v√†o ChromaDB.
    """
    logger.info("--- Starting Vector DB Build Process ---")

    # --- 1. T·∫¢I D·ªÆ LI·ªÜU T·ª™ WIKIPEDIA ---
    logger.info(f"Downloading {len(WIKI_PAGES)} pages from Wikipedia...")
    wiki_api = wikipediaapi.Wikipedia('FailSafeFactChecker/1.0', 'en')
    documents = []
    
    for page_title in tqdm(WIKI_PAGES, desc="Fetching pages"):
        page = wiki_api.page(page_title)
        if page.exists():
            documents.append({
                "title": page_title,
                "text": page.text,
                "url": page.fullurl
            })
        else:
            logger.warning(f"Page '{page_title}' not found on Wikipedia.")

    if not documents:
        logger.error("No documents were fetched. Aborting.")
        return

    # --- 2. CHIA NH·ªé VƒÇN B·∫¢N (CHUNK) ---
    logger.info("Splitting documents into smaller chunks...")
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=100,
        length_function=len
    )
    
    all_chunks = []
    all_metadatas = []
    for doc in tqdm(documents, desc="Splitting text"):
        chunks = text_splitter.split_text(doc['text'])
        for chunk in chunks:
            all_chunks.append(chunk)
            all_metadatas.append({
                "source": doc['url'],
                "title": doc['title']
            })

    # --- 3. KH·ªûI T·∫†O CHROMA DB V√Ä EMBEDDING MODEL ---
    logger.info(f"Initializing embedding model: '{EMBEDDING_MODEL_NAME}'")
    model = SentenceTransformer(EMBEDDING_MODEL_NAME)
    
    logger.info(f"Setting up ChromaDB client at path: '{DB_PATH}'")
    client = chromadb.PersistentClient(path=DB_PATH)
    
    logger.info(f"Creating or getting collection: '{COLLECTION_NAME}'")
    # S·ª≠ d·ª•ng model embedding c·ªßa sentence-transformers
    from chromadb.utils import embedding_functions
    sentence_transformer_ef = embedding_functions.SentenceTransformerEmbeddingFunction(model_name=EMBEDDING_MODEL_NAME)
    
    collection = client.get_or_create_collection(
        name=COLLECTION_NAME,
        embedding_function=sentence_transformer_ef
    )

    # --- 4. T·∫†O VECTOR V√Ä L∆ØU V√ÄO DB ---
    logger.info(f"Generating embeddings and adding {len(all_chunks)} chunks to the database...")
    
    # ChromaDB x·ª≠ l√Ω vi·ªác t·∫°o embedding t·ª± ƒë·ªông khi b·∫°n cung c·∫•p embedding_function
    # Ch√∫ng ta ch·ªâ c·∫ßn cung c·∫•p vƒÉn b·∫£n.
    # T·∫°o ID duy nh·∫•t cho m·ªói chunk
    chunk_ids = [str(i) for i in range(len(all_chunks))]
    
    # Chia th√†nh c√°c batch nh·ªè ƒë·ªÉ th√™m v√†o DB, tr√°nh qu√° t·∫£i b·ªô nh·ªõ
    batch_size = 100
    for i in tqdm(range(0, len(all_chunks), batch_size), desc="Adding to DB"):
        collection.add(
            documents=all_chunks[i:i + batch_size],
            metadatas=all_metadatas[i:i + batch_size],
            ids=chunk_ids[i:i + batch_size]
        )

    logger.info("--- Creating Screening Knowledge Collection ---")
    try:
        # C·ªë g·∫Øng l·∫•y collection, n·∫øu ch∆∞a c√≥ th√¨ t·∫°o m·ªõi
        screening_collection = client.get_or_create_collection(
            name="screening_knowledge",
            embedding_function=sentence_transformer_ef 
        )
        logger.info(f"Successfully created or got 'screening_knowledge' collection. Current count: {screening_collection.count()}")
    except Exception as e:
        logger.error(f"Could not create/get 'screening_knowledge' collection: {e}")

    logger.info("--- Vector DB Build Process Finished! ---")
    logger.info(f"Total chunks indexed: {collection.count()}")


if __name__ == "__main__":
    build_database()